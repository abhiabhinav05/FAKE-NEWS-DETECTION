{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPU2TDr3mumlWn+phBjjmXh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Import libraries\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import PassiveAggressiveClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","# Load datasets\n","# Use 'engine=\"python\"' and 'on_bad_lines=\"skip\"' to handle potential parsing errors in the CSV files.\n","# This is particularly useful for files with inconsistent quoting or newlines within fields.\n","try:\n","    fake = pd.read_csv('Fake.csv', engine='python', on_bad_lines='skip')\n","    real = pd.read_csv('True.csv', engine='python', on_bad_lines='skip')\n","except FileNotFoundError:\n","    print(\"Error: Dataset files 'Fake.csv' or 'True.csv' not found.\")\n","    print(\"Please ensure the files are in the correct directory.\")\n","    # Exit the script or handle the error as needed\n","    exit()\n","\n","\n","# Add labels: 0 = FAKE, 1 = REAL\n","fake['label'] = 0\n","real['label'] = 1\n","\n","# Combine and shuffle\n","data = pd.concat([fake, real])\n","data = data.sample(frac=1).reset_index(drop=True)\n","\n","# Clean text\n","def clean_text(text):\n","    # Ensure text is a string before applying regex\n","    if isinstance(text, str):\n","        text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n","        text = text.lower()  # Convert to lowercase\n","    else:\n","        # Handle non-string values, e.g., convert to empty string\n","        text = \"\"\n","    return text\n","\n","\n","data['text'] = data['text'].apply(clean_text)\n","\n","# Split features and labels\n","X = data['text']\n","y = data['label']\n","\n","# Vectorize using TF-IDF\n","vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n","X_vec = vectorizer.fit_transform(X)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n","\n","# Train classifier\n","model = PassiveAggressiveClassifier(max_iter=50)\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Evaluate\n","acc = accuracy_score(y_test, y_pred)\n","cm = confusion_matrix(y_test, y_pred)\n","\n","print(f\"Accuracy: {acc*100:.2f}%\")\n","print(\"Confusion Matrix:\")\n","print(cm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGGeCdy1VAhs","executionInfo":{"status":"ok","timestamp":1749651545164,"user_tz":-330,"elapsed":15989,"user":{"displayName":"Abhi Abhinav","userId":"01316979554927388890"}},"outputId":"fa50e9f2-6899-4087-a4ba-87ff0051ea45"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 99.39%\n","Confusion Matrix:\n","[[4725   28]\n"," [  27 4200]]\n"]}]}]}